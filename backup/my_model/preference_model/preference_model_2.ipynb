{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67555dd6-8e10-4ad1-8fec-0a6ca5ffa9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81dd3e3c-4654-4d61-a71a-781c19eaa235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention_softmax(nn.Module):\n",
    "    def __init__(self,key_size,query_size,hidden_size,dropout):\n",
    "        super(BahdanauAttention_softmax,self).__init__()\n",
    "        self.wk=nn.Linear(key_size,hidden_size)\n",
    "        self.wq=nn.Linear(query_size,hidden_size)\n",
    "        self.wv=nn.Linear(hidden_size,1)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,queries,keys,cum_subs_masks):\n",
    "        '''\n",
    "        queries:(bs,sq,embs)\n",
    "        keys:(bs,sq,7,hidden_size)\n",
    "        values(bs,sq,7,embs)\n",
    "        cum_subs_masks:(bsm,sq,7,1)\n",
    "        '''\n",
    "        queries,keys=self.dropout(queries),self.dropout(keys)\n",
    "\n",
    "        queries=queries.unsqueeze(2)#(bs,sq,1,embs)\n",
    "        scores=self.wv(F.tanh(self.wq(queries)+self.wk(keys)))#(bs,sq,1,hidden_size广播bs,sq,7,hidden_size)+(bs,sq,7,hidden_size)最后->(bs,sq,7,1)\n",
    "        if cum_subs_masks is not None:\n",
    "            scores+=(cum_subs_masks*-1e9)\n",
    "        attn_weights=F.softmax(scores,dim=-2)#(bs,sq,7,1)\n",
    "        attn_out=attn_weights*keys#(bs,sq,7,1)*(bs,sq,7,hidden_size)->(bs,sq,7,hidden_size)\n",
    "        attn_out=torch.sum(attn_out,dim=-2)#(bs,sq,hidden_size)\n",
    "        return attn_out,attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f02ba2-779b-4809-84cc-bb76a21779f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,mlp_units,mlp_size,acti=torch.relu,rate=0.1):\n",
    "        super(MLP,self).__init__()\n",
    "        self.dropout=nn.Dropout(rate)\n",
    "\n",
    "        self.layernorm1=nn.LayerNorm(mlp_units[0],eps=1e-6)\n",
    "        self.layernorm2=nn.LayerNorm(mlp_units[1],eps=1e-6)\n",
    "\n",
    "        self.dense1=nn.Linear(mlp_size,mlp_units[0])\n",
    "        self.dense2=nn.Linear(mlp_units[0],mlp_units[1])\n",
    "        self.dense_score=nn.Linear(mlp_units[1],mlp_units[2])\n",
    "        self.acti1=acti\n",
    "        self.acti2=F.sigmoid\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x:[bs,sq,hidden_size+embs_size]#x是attn concat queries结果，attn之前对keys做了dense\n",
    "        '''\n",
    "        x=self.dropout(x)\n",
    "        x=self.acti1(self.dense1(x))\n",
    "        x=self.layernorm1(x)\n",
    "\n",
    "        x=self.dropout(x)\n",
    "        x=self.acti1(self.dense2(x))\n",
    "        x=self.layernorm2(x)\n",
    "\n",
    "        y=self.dense_score(x)\n",
    "        y=self.acti2(y)#(bs,sq,1)\n",
    "        y=torch.squeeze(y,dim=-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83d38f61-fe71-430c-a43f-63a822929ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preference_Model(nn.Module):\n",
    "    '''\n",
    "    所用到的数据:\n",
    "                POI id\n",
    "                category id,原文中一级，此处用一级，否则丢失信息\n",
    "                day of week\n",
    "                slot\n",
    "                geohash\n",
    "                **没有**使用用户id\n",
    "    embed->pooling->dense->att->mlp\n",
    "    '''\n",
    "    #embed,使用和sequential model一样的embed_size\n",
    "    def __init__(self,embed_size_poi,embed_size_cat_id,\n",
    "                 embed_size_days,embed_size_slot,embed_size_geohsh,\n",
    "                 hidden_size,\n",
    "                 size_cat_id,size_poi,size_days,size_slots,size_geohshs,\n",
    "                 num_layers,size_tgt,\n",
    "                 mlp_units,dropout=0.1):\n",
    "        super(Preference_Model,self).__init__()\n",
    "\n",
    "        embs_size=embed_size_poi+embed_size_cat_id+embed_size_days+embed_size_slot+embed_size_geohsh\n",
    "        mlp_size=hidden_size+embs_size\n",
    "        \n",
    "        self.embed_poi=nn.Embedding(size_poi,embed_size_poi)\n",
    "        self.embed_cats_id=nn.Embedding(size_cat_id,embed_size_cat_id)\n",
    "        self.embed_days=nn.Embedding(size_days,embed_size_days)\n",
    "        self.embed_slots=nn.Embedding(size_slots,embed_size_slot)\n",
    "        self.embed_geohshs=nn.Embedding(size_geohshs,embed_size_geohsh)\n",
    "        self.dense=nn.Linear(embs_size,hidden_size)\n",
    "        self.attn=BahdanauAttention_softmax(hidden_size,embs_size,hidden_size,dropout)\n",
    "        self.mlp=MLP(mlp_units,mlp_size)\n",
    "\n",
    "    def fun_avg_pooling(self,poi_embs,ctxt_embs,subs_days_masks):\n",
    "        '''\n",
    "        根据样本打卡星期x，将样本划分进入不同类别和时间，并池化\n",
    "        poi_embs:(batch_size,seq_len,embs_size_pois)\n",
    "        ctxt_embs:\n",
    "        sub_days_masks=(batch_size,seq_len,7)\n",
    "        return:\n",
    "                avg_subs_days=(batch_size,seq_len,7,embs_size_pois+embs_size_ctxt)\n",
    "        '''\n",
    "        def divide_no_nan(x,y,nan_value=0.0):\n",
    "            mask=y==0\n",
    "            y=torch.where(mask,torch.ones_like(y),y)\n",
    "            result=x/y\n",
    "            result=torch.where(mask,torch.tensor(nan_value),result)\n",
    "            return result\n",
    "        subs_days_masks=subs_days_masks.float()\n",
    "        cum_idx=torch.cumsum(subs_days_masks,dim=1)#用户在某时间步前，在周x打卡个数(batch_size,seqlen,7)\n",
    "        cum_idx_mask=(cum_idx==0.).float()#如果某时间步在该星期x没有打卡，则取1,(batch_size,seqlen,7)\n",
    "        cum_idx_mask=cum_idx_mask.unsqueeze(-1)#扩展一维（bs，sq，7，1）\n",
    "\n",
    "        inputs_embs=torch.cat((poi_embs,ctxt_embs),2)\n",
    "        inputs_embs_expand=inputs_embs.unsqueeze(2)#(bs,sl,1,embs_size)\n",
    "        inputs_subs_idx=subs_days_masks.unsqueeze(3)#(bs,sl,7,1),用户的打卡在星期上的扩展掩码\n",
    "        inputs_embs_expand_subs=inputs_embs_expand*inputs_subs_idx#(bs,sq,7,embs_size)将inputs_embs_expand划分到7类\n",
    "\n",
    "        inputs_subs_idx_cum=torch.cumsum(inputs_subs_idx,dim=1)\n",
    "        inputs_embs_expand_subs_cum=torch.cumsum(inputs_embs_expand_subs,dim=1)\n",
    "        inputs_embs_expand_subs_cum_avg=divide_no_nan(inputs_embs_expand_subs_cum,inputs_subs_idx_cum)\n",
    "\n",
    "        cum_subs_avg=inputs_embs_expand_subs_cum_avg#pooling后的每周的打卡嵌入(bs,sq,7,embs)\n",
    "        cum_subs_mask=cum_idx_mask#(bs,sl,7,1)某时间步在星期x没有打卡则取1\n",
    "        return cum_subs_avg,cum_subs_mask\n",
    "        \n",
    "    def forward(self,inputs_pois,inputs_cats_id,inputs_days,inputs_slots,inputs_geohshs,inputs_subs_days_mask,\n",
    "                inputs_tgt_pois,inputs_tgt_cats_id,inputs_tgt_days,inputs_tgt_slots,inputs_tgt_hshs):\n",
    "        '''\n",
    "        inputs_pois:(batch_size,seq_len_padded)\n",
    "        inputs_second_id:(batch_size,seq_len_padded)\n",
    "\n",
    "        inputs_tgts:(bs,sq)\n",
    "        ...\n",
    "        '''\n",
    "        #1.embed\n",
    "        inputs_pois=self.embed_poi(inputs_pois)#->(batch_size,seq_pad,embed)\n",
    "        inputs_cats_id=self.embed_cats_id(inputs_cats_id)\n",
    "        inputs_days=self.embed_days(inputs_days)\n",
    "        inputs_slots=self.embed_slots(inputs_slots)\n",
    "        inputs_geohshs=self.embed_geohshs(inputs_geohshs)\n",
    "\n",
    "        inputs_tgt_pois=self.embed_poi(inputs_tgt_pois)\n",
    "        inputs_tgt_cats_id=self.embed_cats_id(inputs_tgt_cats_id)\n",
    "        inputs_tgt_days=self.embed_days(inputs_tgt_days)\n",
    "        inputs_tgt_slots=self.embed_slots(inputs_tgt_slots)\n",
    "        inputs_tgt_geohshs=self.embed_geohshs(inputs_tgt_hshs)\n",
    "\n",
    "        poi_embs=torch.cat((inputs_pois,inputs_cats_id),2)\n",
    "        ctxt_embs=torch.cat((inputs_days,inputs_slots,inputs_geohshs),2)\n",
    "        tgt_info_embs=torch.cat((inputs_tgt_pois,inputs_cats_id,inputs_days,inputs_slots,inputs_geohshs),2)\n",
    "        #pooling\n",
    "        cum_subs_avg,cum_subs_mask=self.fun_avg_pooling(poi_embs,ctxt_embs,inputs_subs_days_mask)#embs=512\n",
    "        #pooling->fc\n",
    "        cum_subs_avg=self.dense(cum_subs_avg)\n",
    "        #att\n",
    "        keys=cum_subs_avg#(bs,sq,7,hidden_size)\n",
    "        queries=tgt_info_embs#(bs,sq,embs)\n",
    "        attn_out,attn_w=self.attn(queries,keys,cum_subs_mask)#(bs,sq,hidden_size)\n",
    "        #mlp\n",
    "        concat=torch.cat((attn_out,queries),dim=-1)#(bs,sq,hidden_size)(bs,sq,embs)->(bs,sq,hidden_size+embs_size)\n",
    "        mlp_out=self.mlp(concat)#(bs,seqlen,1)\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "087cecd1-1294-4aae-9d67-d6c030da7e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.08115595579147\n",
      "33.969039499759674\n",
      "33.90558007359505\n",
      "33.884389728307724\n",
      "33.87228274345398\n",
      "33.86445689201355\n",
      "33.85895365476608\n",
      "33.85484632849693\n",
      "33.85162925720215\n",
      "33.84902533888817\n"
     ]
    }
   ],
   "source": [
    "embed_size_poi=300\n",
    "embed_size_cat_id=100\n",
    "embed_size_days=16\n",
    "embed_size_slot=32\n",
    "embed_size_geohsh=64\n",
    "\n",
    "hidden_size=128\n",
    "num_layers=2\n",
    "epoch_size=10\n",
    "batch_size=10\n",
    "mlp_units=[256,128,1]\n",
    "lr=0.0001\n",
    "\n",
    "size_cat_id=285#虽然FourSquare官网有约1k种，但该数据集中出现285中，0-284\n",
    "size_poi=3906#0-3906\n",
    "size_days=8#原取值1-7，做embed取8\n",
    "size_slots=25#1-24\n",
    "size_geohsh=95#0-94\n",
    "size_tgt=3906\n",
    "\n",
    "#1.1用padded数据\n",
    "'''\n",
    "不是rec数据而是所有的数据\n",
    "'''\n",
    "dir_inputs_pois='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_reindex_poi.pkl'\n",
    "dir_inputs_cats_id='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_reindex_cat_id.pkl'#使用最小目录否则丢失信息\n",
    "dir_inputs_days='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_days.pkl'\n",
    "dir_inputs_slots='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_hours.pkl'\n",
    "dir_inputs_geohshs='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_hs5.pkl'\n",
    "dir_inputs_subs_days='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_subs_days.pkl'\n",
    "\n",
    "dir_inputs_tgt_pois='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_tgt_reindex_poi.pkl'\n",
    "dir_inputs_tgt_cats_id='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_tgt_reindex_cat_id.pkl'\n",
    "dir_inputs_tgt_days='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_tgt_days.pkl'\n",
    "dir_inputs_tgt_slots='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_tgt_slots.pkl'\n",
    "dir_inputs_tgt_geohshs='/home/jovyan/datasets/tsmc_nyc_8_all_tgt_padding/padded_tgt_hshs.pkl'\n",
    "\n",
    "\n",
    "with open(dir_inputs_pois,'rb') as f:\n",
    "    inputs_pois=pickle.load(f)\n",
    "with open(dir_inputs_cats_id,'rb') as f:\n",
    "    inputs_cats_id=pickle.load(f)\n",
    "with open(dir_inputs_days,'rb') as f:\n",
    "    inputs_days=pickle.load(f)\n",
    "with open(dir_inputs_slots,'rb') as f:\n",
    "    inputs_slots=pickle.load(f)\n",
    "with open(dir_inputs_geohshs,'rb') as f:\n",
    "    inputs_geohshs=pickle.load(f)\n",
    "with open(dir_inputs_subs_days,'rb') as f:\n",
    "    inputs_subs_days_mask=pickle.load(f)\n",
    "    \n",
    "with open(dir_inputs_tgt_pois,'rb') as f:\n",
    "    inputs_tgt_pois=pickle.load(f)\n",
    "with open(dir_inputs_tgt_cats_id,'rb') as f:\n",
    "    inputs_tgt_cats_id=pickle.load(f)\n",
    "with open(dir_inputs_tgt_days,'rb') as f:\n",
    "    inputs_tgt_days=pickle.load(f)\n",
    "with open(dir_inputs_tgt_slots,'rb') as f:\n",
    "    inputs_tgt_slots=pickle.load(f)\n",
    "with open(dir_inputs_tgt_geohshs,'rb') as f:\n",
    "    inputs_tgt_geohshs=pickle.load(f)\n",
    "#1.2写dataloader\n",
    "def Process_preference_model(padded_pois,padded_cats_id,padded_days,padded_slots,\n",
    "                            padded_geohshs,padded_subs_days,\n",
    "                             padded_tgt_pois,padded_tgt_cats_id,padded_tgt_days,padded_tgt_slots,padded_tgt_geohshs,batch_size):\n",
    "    class Pre_Dataset(Dataset):\n",
    "        def __init__(self,padded_pois,padded_cats_id,padded_days,padded_slots,\n",
    "                     padded_geohshs,padded_subs_days_mask,inputs_tgt_pois,inputs_tgt_cats_id,inputs_tgt_days,inputs_tgt_slots,inputs_tgt_geohshs):\n",
    "            self.padded_pois=padded_pois\n",
    "            self.padded_cats_id=padded_cats_id\n",
    "            self.padded_days=padded_days\n",
    "            self.padded_slots=padded_slots\n",
    "            self.padded_geohshs=padded_geohshs\n",
    "            self.padded_subs_days_mask=padded_subs_days_mask\n",
    "            self.padded_tgt_pois=padded_tgt_pois\n",
    "            self.padded_tgt_cats_id=padded_tgt_cats_id\n",
    "            self.padded_tgt_days=padded_tgt_days\n",
    "            self.padded_tgt_slots=padded_tgt_slots\n",
    "            self.padded_tgt_geohshs=padded_tgt_geohshs\n",
    "        def __len__(self):\n",
    "            return len(self.padded_pois)\n",
    "        def __getitem__(self,index):\n",
    "            pois=self.padded_pois[index]\n",
    "            cats_id=self.padded_cats_id[index]\n",
    "            days=self.padded_days[index]\n",
    "            slots=self.padded_slots[index]\n",
    "            geohshs=self.padded_geohshs[index]\n",
    "            subs_days_mask=self.padded_subs_days_mask[index]\n",
    "            tgt_pois=self.padded_tgt_pois[index]\n",
    "            tgt_cats_id=self.padded_tgt_cats_id[index]\n",
    "            tgt_days=self.padded_tgt_days[index]\n",
    "            tgt_slots=self.padded_tgt_slots[index]\n",
    "            tgt_geohshs=self.padded_tgt_geohshs[index]\n",
    "            return pois,cats_id,days,slots,geohshs,subs_days_mask,tgt_pois,tgt_cats_id,tgt_days,tgt_slots,tgt_geohshs\n",
    "    #创建数据集实例\n",
    "    all_datasets=Pre_Dataset(padded_pois,padded_cats_id,padded_days,padded_slots,\n",
    "                            padded_geohshs,padded_subs_days,\n",
    "                             padded_tgt_pois,padded_tgt_cats_id,padded_tgt_days,padded_tgt_slots,padded_tgt_geohshs)\n",
    "    dataloader=DataLoader(all_datasets,batch_size,shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "pre_all_data=Process_preference_model(inputs_pois,inputs_cats_id,inputs_days,inputs_slots,\n",
    "                                      inputs_geohshs,inputs_subs_days_mask,\n",
    "                                      inputs_tgt_pois,inputs_tgt_cats_id,inputs_tgt_days,inputs_tgt_slots,inputs_tgt_geohshs,batch_size)\n",
    "#1.3传入数据\n",
    "model=Preference_Model(embed_size_poi,embed_size_cat_id,\n",
    "                       embed_size_days,embed_size_slot,embed_size_geohsh,\n",
    "                       hidden_size,\n",
    "                       size_cat_id,size_poi,size_days,size_slots,size_geohsh,\n",
    "                       num_layers,size_tgt,mlp_units)\n",
    "\n",
    "\n",
    "loss_function=nn.BCEWithLogitsLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr)\n",
    "\n",
    "for epoch in range(epoch_size):\n",
    "    model.train()\n",
    "    epoch_loss=0.0\n",
    "    for batch_step,(batch_pois,batch_cats_id,batch_days,batch_slots,batch_geohshs,padded_subs_days_mask,\n",
    "                    batch_tgt_pois,batch_tgt_cats_id,tgt_days,tgt_slots,tgt_hshs) in enumerate(pre_all_data):\n",
    "        model.zero_grad()\n",
    "        loss=0.0\n",
    "        outputs=model(batch_pois,batch_cats_id,batch_days,batch_slots,\n",
    "                      batch_geohshs,padded_subs_days_mask,\n",
    "                      batch_tgt_pois,batch_tgt_cats_id,tgt_days,tgt_slots,tgt_hshs)#(bs,sq)\n",
    "        #计算loss\n",
    "        outputs=outputs.unsqueeze(-1)\n",
    "        labels=torch.ones_like(outputs)\n",
    "        loss+=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss+=float(loss)\n",
    "        # print(loss)\n",
    "        # break#只执行一个循环\n",
    "    print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304d2de-bff9-43eb-97a0-939600b40e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
