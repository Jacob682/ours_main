{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f66f3248-2fe7-4c23-b352-9ed584027783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch import Tensor\n",
    " \n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25e3a89-c318-4141-acab-a4fed98a659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exe_time(func):\n",
    "    '''\n",
    "    装饰器，返回原函数返回的，并打印出原函数前后的时间\n",
    "    '''\n",
    "    def new_func(*arg,**kwargs):\n",
    "        name=func.__name__\n",
    "        back=func(*args,**kwargs)\n",
    "        start=datetime.datetime.now()\n",
    "        end=datetime.datetime.now()\n",
    "        total=(end-start).total_seconds()\n",
    "        print('--{%s} start:@ %ss' % (name,start))\n",
    "        print('--{%s} end:@ %ss'%(name,end))\n",
    "        print('--{%s} total:@ %.3fs=%.3fh'%(name,total,total/3600.0))\n",
    "        return back\n",
    "    return new_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c07e76-e11b-4044-8d70-c506dd74f922",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STPIL_Basic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSTPIL_Long_attqk\u001b[39;00m(\u001b[43mSTPIL_Basic\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, usr_feas, train, test, long_short, num_inps, dim_feas, num_negs, num_topk,\n\u001b[1;32m      3\u001b[0m                  mlp_units, transformer_params, alpha_lambda, dropout_rate):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(STPIL_Long_attqk, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      5\u001b[0m             usr_feas, train, test, long_short, num_inps, dim_feas, num_negs, num_topk,\n\u001b[1;32m      6\u001b[0m             mlp_units, transformer_params, alpha_lambda, dropout_rate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STPIL_Basic' is not defined"
     ]
    }
   ],
   "source": [
    "class STPIL_Long_attqk(STPIL_Basic):\n",
    "    def __init__(self, usr_feas, train, test, long_short, num_inps, dim_feas, num_negs, num_topk,\n",
    "                 mlp_units, transformer_params, alpha_lambda, dropout_rate):\n",
    "        super(STPIL_Long_attqk, self).__init__(\n",
    "            usr_feas, train, test, long_short, num_inps, dim_feas, num_negs, num_topk,\n",
    "            mlp_units, transformer_params, alpha_lambda, dropout_rate)\n",
    "        # trainable_variables from outer classes\n",
    "        self.model_mlp = MLP(self.mlp_units, self.acti, self.rate_mlp)\n",
    "        self.dense = nn.Linear(self.d_model, self.d_model)  # 调控长期兴趣的维度。\n",
    "        self.model_att_qk_sft = BahdanauAttentionQK_softmax(self.d_model, self.rate_long)  # 多兴趣聚合\n",
    "\n",
    "    def forward(self, tra_elements):\n",
    "        loss = self.batch_model_tra(tra_elements)\n",
    "        return loss\n",
    "        \n",
    "    def batch_model_tra(self, tra_elements):  # 变量都是batch形式,tra_elements\n",
    "        # input\n",
    "        # ===============================================================\n",
    "        # input: idx\n",
    "        [\n",
    "            # (sz, 1), (sz, len+1), (sz, len+1, 20), (sz, len+1, 7/9), (sz, len+1, 5/5/5/5)\n",
    "            u_idxs,\n",
    "            tra_pois_idxs, tra_cats_idxs, tra_1sts_idxs, tra_days_idxs, tra_slts_idxs, tra_hs5s_idxs, tra_msks,\n",
    "            tra_pois_negs_idxs, tra_cats_negs_idxs, tra_1sts_negs_idxs,\n",
    "            tra_subs_days_idxs, tra_subs_1sts_idxs,\n",
    "            tra_rect_timh_idxs, tra_rect_dist_idxs, tra_rect_timv_idxs, tra_rect_ctxt_idxs,\n",
    "            tra_long, tra_short  # (sz, len+1, 7/9), (sz, len+1, 5)\n",
    "        ] = tra_elements\n",
    "        # input: emb    # 这些用于算loss\n",
    "        # u_embs = self.emb_layer_usr(u_idxs)                   # (sz, 1, 128)\n",
    "        tra_pois_embs = self.emb_layer_poi(tra_pois_idxs)  # (sz, len+1, 128)\n",
    "        tra_cats_embs = self.emb_layer_cat(tra_cats_idxs)  # (sz, len+1, 64)\n",
    "        # tra_1sts_embs = self.emb_layer_1st(tra_1sts_idxs)     # (sz, len+1, 16)\n",
    "        tra_days_embs = self.emb_layer_day(tra_days_idxs)  # (sz, len+1, 16)\n",
    "        tra_slts_embs = self.emb_layer_slt(tra_slts_idxs)  # (sz, len+1, 32)\n",
    "        tra_hs5s_embs = self.emb_layer_hs5(tra_hs5s_idxs)  # (sz, len+1, 64)\n",
    "        tra_pois_negs_embs = self.emb_layer_poi(tra_pois_negs_idxs)  # (sz, len+1, 10, 128)\n",
    "        tra_cats_negs_embs = self.emb_layer_cat(tra_cats_negs_idxs)\n",
    "        # tra_1sts_negs_embs = self.emb_layer_1st(tra_1sts_negs_idxs)\n",
    "        # input: concatenate\n",
    "        # ===============================================================\n",
    "        batch_sz = tra_msks.size(0)\n",
    "        tra_len_plus = tra_msks.size(1)\n",
    "        # 用户的：shape=(sz, len+1, 21\\len_q, 128)\n",
    "        # tra_user_embs = u_embs.repeat(1, tra_len_plus, 1)   # (sz, len+1, 128)\n",
    "        # tra_user_embs = tra_user_embs.unsqueeze(2).repeat(1, 1, 1+self.num_negs, 1)\n",
    "        # 正样本：shape=(sz, len+1, 208)\n",
    "        tra_posi_embs = torch.cat([tra_pois_embs, tra_cats_embs], dim=-1)  # , tra_1sts_embs\n",
    "        # 负样本：shape=(sz, len+1, 10, 208)\n",
    "        tra_nega_embs = torch.cat([tra_pois_negs_embs, tra_cats_negs_embs], dim=-1)  # , tra_1sts_negs_embs\n",
    "        # 上下文：shape=(sz, len+1, 112)\n",
    "        tra_ctxt_embs = torch.cat([tra_days_embs, tra_slts_embs, tra_hs5s_embs], dim=-1)\n",
    "        # 长兴趣：shape=(sz, len+1, 7/9)\n",
    "        # tra_long_idxs = tra_long.type(tra_pois_embs.dtype)\n",
    "        # mask：shape=(sz, len+1)\n",
    "        tra_msks = tra_msks.type(tra_pois_embs.dtype)\n",
    "        # label：shape=(sz, len+1, 21)\n",
    "        labels = torch.tensor([[0]])  # 第0列是1表示正样本。\n",
    "        n_classes = torch.tensor(self.num_negs + 1)\n",
    "        labels_one_hot = torch.nn.functional.one_hot(labels, n_classes)  # (1, 1, 21),\n",
    "        labels_one_hot = labels_one_hot.repeat(batch_sz, tra_len_plus, 1)\n",
    "        labels_one_hot = labels_one_hot.type(tra_pois_embs.dtype)  # (batch_sz, len+1, 21)\n",
    "\n",
    "        '''\n",
    "        预测过程：基于各种特征预测在当前[t+1]时刻poi，训练时基于[t+1]时的正负样本。\n",
    "            input_seq是[1, len-1]时刻的真值。\n",
    "            target_seq是[2, len]时刻的真值, mask也是在这些时刻\n",
    "            top_idx_sorted预测的是[2, len]时刻的真值\n",
    "            后边各种计算要根据这个[1, len-1] → [2, len]来调整输入的维度。= [a,b,c] → [b,c,d]\n",
    "            \n",
    "            用[0,a,b,c,d]预测[b,c,d], 因为最左侧补了零列用于短期兴趣从seq中取数据。\n",
    "        tra:\n",
    "            len_q = 1+num_neg\n",
    "            len_k = 7/9, days/1sts\n",
    "            q.shape = (batch_sz, len+1, len_q, d_model) = (sz, len+1, 21, 128)\n",
    "            k.shape = (batch_sz, len+1, len_k, d_model) = (sz, len+1, 7/9, 128)\n",
    "            v.shape = (batch_sz, len+1, len_k, d_model) = (sz, len+1, 7/9, 128)\n",
    "            mask.shape = (batch_sz, len+1, 1, 7/9) = (sz, len+1, 1, 7/9)\n",
    "        '''\n",
    "        tra_long = tra_long.unsqueeze(2)  # [1, len-1, 7/9] → [1, len-1, 1, 7/9]\n",
    "        long_feas = self.dense(tra_long)  # (sz, len+1, 1, 7/9) → (sz, len+1, 1, 128)\n",
    "        # key, query, value\n",
    "        tra_q = torch.cat([tra_posi_embs, long_feas], dim=-2)  # (sz, len+1, len_q, 128)\n",
    "        tra_k = tra_ctxt_embs.unsqueeze(2).repeat(1, 1, self.num_negs + 1, 1)  # (sz, len+1, len_k, 128)\n",
    "        tra_v = tra_ctxt_embs.unsqueeze(2).repeat(1, 1, self.num_negs + 1, 1)  # (sz, len+1, len_k, 128)\n",
    "        # mask: 从[c, 1, d]变为[c, d]，变换前的mask是在输入上有效的，可以使用。\n",
    "        mask = tra_msks.unsqueeze(2).repeat(1, 1, self.num_negs + 1)\n",
    "        mask = mask.unsqueeze(3)  # (sz, len+1, 1, 7/9)\n",
    "        mask = mask.type(tra_pois_embs.dtype)\n",
    "\n",
    "        '''\n",
    "        长期兴趣特征整合。\n",
    "        '''\n",
    "        # Attention权重\n",
    "        att_wts = self.model_att_qk_sft(tra_q, tra_k, tra_v, mask)  # (sz, len+1, len_q, d_model)\n",
    "        # (sz, len+1, len_q, d_model) → (sz, len+1, d_model)\n",
    "        tra_ctxt_2nd = torch.einsum('ijkl,ijlm->ijkm', att_wts, tra_v)  # (sz, len+1, 1, d_model)\n",
    "        tra_ctxt_2nd = tra_ctxt_2nd.squeeze(2)  # (sz, len+1, d_model)\n",
    "\n",
    "        '''\n",
    "        模型结构。\n",
    "        '''\n",
    "        # mlp\n",
    "        # context_2nd = torch.cat([ctxt_embs, ctxt_2nd], dim=-1)\n",
    "        out_ctxt_2nd = self.model_mlp(tra_ctxt_2nd)  # (sz, len+1, 256)\n",
    "        out_ctxt_2nd = out_ctxt_2nd.unsqueeze(2).repeat(1, 1, self.num_negs + 1, 1)  # (sz, len+1, 21, 256)\n",
    "        out_posi_embs = tra_posi_embs.unsqueeze(2).repeat(1, 1, self.num_negs + 1, 1)  # (sz, len+1, 21, 208)\n",
    "        # concatenate\n",
    "        out_ctxt_2nd = torch.cat([out_ctxt_2nd, out_posi_embs], dim=-1)  # (sz, len+1, 21, 464)\n",
    "        # predict\n",
    "        logits = self.model_dense(out_ctxt_2nd)  # (sz, len+1, 21, 1)\n",
    "        logits = logits.squeeze(-1)  # (sz, len+1, 21)\n",
    "        logits = logits * labels_one_hot  # (sz, len+1, 21)\n",
    "        loss = -torch.log(torch.sigmoid(logits)).sum()  # 注意是求和，而不是均值。loss由正样本+负样本构成。\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7009a7-0931-4a3e-87fd-52d6837d2595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preference_Model(nn.Module):\n",
    "    def __init__(self,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743dd0d7-5dc8-42d5-8e20-a412d786cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_pre(dataset_path,split=-1,num_negs=5):\n",
    "    '''\n",
    "    区别于Process_rec,直接传入路径，而不是在外面读完，传入路径\n",
    "    '''\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
