{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9358e2-91d1-4eb3-ad27-b938b5a18828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class RecPreferenceModel(nn.Module):\n",
    "    def __init__(self,embed_size_user,embed_size_cat,hidden_size,\n",
    "                size_user,size_cat,size_poi,\n",
    "                num_layers,size_tgt,dropout=0.1):\n",
    "        super(RecPreferenceModel,self).__init__()\n",
    "        \n",
    "        self.embed_user=nn.Embedding(size_user,embed_size_user)\n",
    "        self.embed_cat=nn.Embedding(size_cat,embed_size_cat)\n",
    "        \n",
    "        lstm_size=embed_size_user+embed_size_cat+2#delta_t，delta_d维度\n",
    "        self.lstm=nn.LSTM(lstm_size,hidden_size,num_layers,batch_first=True,dropout=dropout)\n",
    "        self.fc=nn.Linear(hidden_size,size_tgt)\n",
    "    \n",
    "    def forward(self,inputs_user,inputs_cat,inputs_delta_t,inputs_delta_d):\n",
    "        '''\n",
    "        inputs_cat、inputs_delta_t,inputs_delta_d是padding之后的\n",
    "        '''\n",
    "        inputs_user=self.embed_user(inputs_user).unsqueeze(1).repeat(1,inputs_cat.size(1),1)#(n_user,1)->(n_user,seq_len,1)\n",
    "        inputs_cat=self.embed_cat(inputs_cat)#(n_user,seq_len,embed_cat)\n",
    "        inputs_delta_t=inputs_delta_t.unsqueeze(2)#(n_users,seq_len,1)\n",
    "        inputs_delta_d=inputs_delta_d.unsqueeze(2)#(n_users,seq_len,1)\n",
    "        inputs=torch.cat((inputs_user,inputs_cat,inputs_delta_t,inputs_delta_d),2)\n",
    "        output,_=self.lstm(inputs)\n",
    "        out=self.fc(output)#注掉之后相当于直接输出隐藏层,没注释掉之后就是还是tgt个输出,最后用second_importance在相应位置上做权重\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1605dc-831a-4d89-9cb8-d5e3687d9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备数据\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "dir_user='/home/jovyan/datasets/tsmc_nyc_3_groupby_user_chronological/user_id.pkl'\n",
    "dir_padded_reindex_rec_cat_id='/home/jovyan/datasets/tsmc_nyc_7_rec_tgt_padding/padded_reindex_rec_cat_id.pkl'\n",
    "dir_padded_rec_delta_t='/home/jovyan/datasets/tsmc_nyc_7_rec_tgt_padding/padded_rec_delta_t.pkl'\n",
    "dir_padded_rec_delta_d='/home/jovyan/datasets/tsmc_nyc_7_rec_tgt_padding/padded_rec_delta_d.pkl'\n",
    "with open(dir_user,'rb') as f:\n",
    "    users=pickle.load(f)\n",
    "users=torch.IntTensor(users)\n",
    "\n",
    "with open(dir_padded_reindex_rec_cat_id,'rb') as f:\n",
    "    padded_reindex_rec_cat_id=pickle.load(f)\n",
    "\n",
    "with open(dir_padded_rec_delta_t,'rb') as f:\n",
    "    padded_rec_delta_t=pickle.load(f)\n",
    "\n",
    "with open(dir_padded_rec_delta_d,'rb') as f:\n",
    "    padded_rec_delta_d=pickle.load(f)\n",
    "\n",
    "embed_size_user=50\n",
    "embed_size_cat=100\n",
    "hidden_size=128\n",
    "size_user=max(users)+1\n",
    "size_cat=285\n",
    "size_poi=3906\n",
    "num_layers=1\n",
    "batch_size=5\n",
    "epoch_size=25\n",
    "lr=0.001\n",
    "#声明模型\n",
    "rec_pre_model=RecPreferenceModel(embed_size_user,embed_size_cat,hidden_size,\n",
    "size_user,size_cat,size_poi,num_layers,size_cat)\n",
    "# out=rec_pre_model(users,padded_reindex_rec_cat_id,padded_rec_delta_t,padded_rec_delta_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be819b5-a140-4d7d-ab41-c1a543334a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.将数据给dataloader\n",
    "##1.1定义dataloader,已经padding之后再给dataloader，\n",
    "##最后放入模型,模型里面embed\n",
    "def Process_rec(users,padded_reindex_rec_cat_id,\n",
    "                padded_rec_delta_t,padded_rec_delta_d,\n",
    "                tgt,\n",
    "                batch_size):\n",
    "    '''\n",
    "    users:(n_user,1)\n",
    "    padded_reindex_rec_cat_id:(n_user,20,embed_cat_size)\n",
    "    padded_rec_delta_t/d:(n_user,20),不需要embed，最后在模型里concat\n",
    "    '''\n",
    "    class RecDataset(Dataset):\n",
    "        def __init__(self,users,padded_reindex_rec_cat_id,padded_rec_delta_t,padded_rec_delta_d,tgt):\n",
    "            self.users=users\n",
    "            self.padded_reindex_rec_cat_id=padded_reindex_rec_cat_id\n",
    "            self.padded_rec_delta_t=padded_rec_delta_t\n",
    "            self.padded_rec_delta_d=padded_rec_delta_d\n",
    "            self.tgt=tgt\n",
    "        def __len__(self):\n",
    "            return len(self.users)#长度相同为所有用户个数\n",
    "        def __getitem__(self,index):#得到每一个用户的数据，非每个时间步，即每个用户的打卡数据\n",
    "            user=self.users[index]\n",
    "            cat_id=self.padded_reindex_rec_cat_id[index]\n",
    "            delta_t=self.padded_rec_delta_t[index]\n",
    "            delta_d=self.padded_rec_delta_d[index]\n",
    "            tgt_item=self.tgt[index]\n",
    "            return user,cat_id,delta_t,delta_d,tgt_item\n",
    "    #创建数据集实例\n",
    "    rec_dataset=RecDataset(users,padded_reindex_rec_cat_id,padded_rec_delta_t,padded_rec_delta_d,tgt)\n",
    "    #创建dataloader\n",
    "    dataloader=DataLoader(rec_dataset,batch_size,shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b22ff3-0cc3-4bbd-8172-2f3824218fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##1.2tgt_cat,tgt_poi数据\n",
    "dir_reindex_tgt=['/home/jovyan/datasets/tsmc_nyc_7_rec_tgt_padding/padded_tgt_reindex_rec_cat_id.pkl',\n",
    "                 '/home/jovyan/datasets/tsmc_nyc_7_rec_tgt_padding/padded_tgt_reindex_rec_poi.pkl']\n",
    "# reindex_tgt_name=['tgt_reindex_rec_cat_id','tgt_reindex_rec_poi']\n",
    "with open(dir_reindex_tgt[0],'rb') as f:\n",
    "    padded_tgt_reindex_rec_cat_id=pickle.load(f)\n",
    "with open(dir_reindex_tgt[1],'rb') as f:\n",
    "    padded_tgt_reindex_rec_poi=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29925f95-cbd1-4515-8e1f-66e44db79519",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1.3将数据传给dataloader\n",
    "##传cat数据试试\n",
    "pre_data=Process_rec(users,padded_reindex_rec_cat_id,padded_rec_delta_t,padded_rec_delta_d,padded_tgt_reindex_rec_cat_id,batch_size)#,tgt_reindex_rec_cat_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca56a8a-2b54-4330-b0af-50fc940a2a97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1078])\n",
      "torch.Size([1078, 20])\n",
      "torch.Size([1078, 20])\n"
     ]
    }
   ],
   "source": [
    "#查看size\n",
    "print(users.size())\n",
    "print(padded_reindex_rec_cat_id.size())\n",
    "print(padded_tgt_reindex_rec_cat_id.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3236b021-80b0-4acd-ab0e-79d4e9d48ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##1.4验证dataloader\n",
    "# for batch_users,batch_cat_id,batch_delta_t,batch_delta_d,batch_tgt_cat_id in pre_data:\n",
    "# # in pre_data:\n",
    "#     print(f'batch data:{batch_users},{batch_cat_id},{batch_delta_t},{batch_delta_d},{batch_tgt_cat_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d22014bd-2b51-4ba5-8bf0-d9f356db41d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2.3定义评价矩阵\n",
    "def Top_k_precision(indices, batch_y, k):\n",
    "    '''\n",
    "    indices:一个batch排序之后的下标，(batch_size,待预测长度),size_cat/size_poi，tensor\n",
    "    batch_y:(batch_size)\n",
    "    '''\n",
    "    precision = 0\n",
    "    for i in range(indices.size(0)):\n",
    "        sort = indices[i]\n",
    "        if batch_y[i] in sort[:k]:\n",
    "            precision += 1\n",
    "    return precision / indices.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f305d26-883e-49b4-9e9f-61f6ab601a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2创建for循环，将数据送入模型\n",
    "#2.1将模型放入cuda\n",
    "rec_pre_model=rec_pre_model.cuda()\n",
    "#2.2创建loss函数\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(rec_pre_model.parameters(),lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c41cc0-53d2-414c-99c6-9a380d510d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3对用户打卡切片得到真实预测的时间步长度\n",
    "def Regain_batch_outputs_len(batch_step,batch_data):\n",
    "    # 读rec打卡长度数据，tgt长度相等\n",
    "    dir_lens='/home/jovyan/datasets/tsmc_nyc_7_rec_tgt_padding/lens.pkl'\n",
    "    with open(dir_lens,'rb') as f:\n",
    "        lens=pickle.load(f)\n",
    "    # lens=torch.full((10,1),5)\n",
    "    batch_data_list=[]\n",
    "    batch_lens=lens[batch_step*(len(batch_data)):(batch_step+1)*(len(batch_data))]\n",
    "    for i in range(len(batch_data)):\n",
    "        batch_data_list.append(batch_data[i][:batch_lens[i]])\n",
    "    return batch_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e46f7d-a93b-42f8-b3a4-e1e454012192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Regain_batch_tgt(batch_step,batch_data):\n",
    "    '''可以是cat_id,poi_id'''\n",
    "    dir_nonpadded_reindex_cat_id='/home/jovyan/datasets/tsmc_nyc_4_recent_target/tgt_reindex_cat_id_poi/tgt_reindex_rec_cat_id.pkl'\n",
    "    dir_nonpadded_reindex_poi='/home/jovyan/datasets/tsmc_nyc_4_recent_target/tgt_reindex_cat_id_poi/tgt_reindex_rec_poi.pkl'\n",
    "    dir_nonpadded_reindex=[dir_nonpadded_reindex_cat_id,dir_nonpadded_reindex_poi]\n",
    "    var_name=['nonpadded_cat_id','nonpadded_poi']\n",
    "    for d,v in zip(dir_nonpadded_reindex,var_name):\n",
    "        with open(d,'rb') as f:\n",
    "            exec(f'{v}={pickle.load(f)}',globals())\n",
    "    batch_cat_id_list=nonpadded_cat_id[batch_step*(len(batch_data)):(batch_step+1)*(len(batch_data))]\n",
    "    batch_poi_list=nonpadded_poi[batch_step*(len(batch_data)):(batch_step+1)*(len(batch_data))]\n",
    "    return batch_cat_id_list,batch_poi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8d03fad-9d4a-4d6c-a5f9-04a00c6ccfde",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3.2测试切片得到的nonpadded tgt\n",
    "# batch_nonpadded_tgt_cat_id,_=Regain_batch_tgt(0,5)\n",
    "# pass\n",
    "# print(batch_nonpadded_tgt_cat_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eb02aba-e639-4e8e-beee-2abc08247dcd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.3873, 0.0378, 0.7474, 0.5494, 0.2460, 0.8468, 0.3971, 0.0268, 0.0103,\n",
      "        0.9155, 0.6697, 0.6641, 0.7250, 0.0793, 0.8628, 0.6236, 0.1172, 0.3212,\n",
      "        0.9099, 0.7425]), tensor([0.5650, 0.3861, 0.7394, 0.8040, 0.5241, 0.2001, 0.9417, 0.1547, 0.8578,\n",
      "        0.1018, 0.4120, 0.1858, 0.7915, 0.1657, 0.1192, 0.6120, 0.7373, 0.9677,\n",
      "        0.5816, 0.3606]), tensor([0.7286, 0.8253, 0.9361, 0.6999, 0.9564, 0.4443, 0.4343, 0.8315, 0.6362,\n",
      "        0.5845, 0.4948, 0.5100, 0.1715, 0.0270, 0.9174, 0.1582, 0.9338, 0.2018,\n",
      "        0.1446, 0.6727]), tensor([0.8696, 0.1498, 0.7897, 0.3897, 0.8573, 0.2956, 0.3931, 0.1810, 0.8123,\n",
      "        0.7180, 0.9935, 0.4008, 0.3049, 0.2564, 0.6844, 0.5558, 0.1643, 0.5727,\n",
      "        0.7450, 0.1183]), tensor([0.5808, 0.4362, 0.4287, 0.6609, 0.6377, 0.0987, 0.7599, 0.9235, 0.8036,\n",
      "        0.8966, 0.2059, 0.1727, 0.7240, 0.0122, 0.3372, 0.0560, 0.6540, 0.4721,\n",
      "        0.0919, 0.1514])]\n"
     ]
    }
   ],
   "source": [
    "#3.1测试根据切片得到的真实长度\n",
    "##3.1.1生成（5，20）的tensor数据\n",
    "data=torch.rand(5,20)\n",
    "data=Regain_batch_outputs_len(0,data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926fd3c-c5b2-422a-99cc-2f005ab58a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1对每个时间步输出的结果(1,tgt_size)在想应位置上添加second_importance\n",
    "#1.1读取second_importance数据，读取second和cat_id对应数据\n",
    "dir_second_importance='/home/jovyan/datasets/tsmc_nyc_10_users_timesteps_second_importance/users_timesteps_second_importance.pkl'\n",
    "dir_dic_second_id='/home/jovyan/datasets/tsmc_nyc_6_distinguish_cat_poi/distin_dic_second_cat_id.pkl'\n",
    "dir_dic_cat_id='/home/jovyan/datasets/tsmc_nyc_6_distinguish_cat_poi/distin_dic_cat_id.pkl'\n",
    "dir_dic_id_second='/home/jovyan/datasets/level/foursquare_category_level/foursquare_category_level_v2/dic_id_second.pkl'\n",
    "\n",
    "\n",
    "with open(dir_second_importance,'rb') as f:\n",
    "    second_importance=pickle.load(f)#每个二级目录重要性\n",
    "with open(dir_dic_second_id,'rb') as f:\n",
    "    dic_second_id=pickle.load(f)#二级目录（16进制）：重索引（int）\n",
    "# with open(dir_reindex_second_cat_id,'rb') as f:\n",
    "#     reindex_second_cat_id=pickle.load(f)#每个用户打卡种类的二级目录表示 [users_size,seq_len]\n",
    "\n",
    "with open(dir_dic_id_second,'rb') as f:\n",
    "    dic_id_second=pickle.load(f)#每个二级目录(hex)：目录(hex list)\n",
    "with open(dir_dic_cat_id,'rb') as f:\n",
    "    dic_cat_id=pickle.load(f)#目录(hex)：reindex id(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9bbb5-a3a7-4ba4-bd31-d10e04406065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Checkin_cat2importance(outputs,dic_cat_id,dic_id_second,dic_second_id,second_importance,b,batch_size):\n",
    "    '''\n",
    "    对tgt_size根据位置赋不同的权重,从而修改lstm_outputs\n",
    "    outputs:(batch_size,seq_len,tgt_size)\n",
    "    dic_cat_id:hex:reindex(int)\n",
    "    dic_id_second:hex:hex list\n",
    "    dic_second_id:hex: second_reindex(int)\n",
    "    second_importance:(users,seq,second_size)\n",
    "    b:第几个batch\n",
    "    '''\n",
    "    def Find_key_by_value_121(v,values,keys):\n",
    "        index=values.index(v)\n",
    "        return keys[index]\n",
    "\n",
    "    def Find_key_by_value_12many(v,dic):\n",
    "        for key,val_list in dic.items():\n",
    "            if v in val_list:\n",
    "                return key\n",
    "        \n",
    "    def Find_second_by_cat_id(cat_id,second_keys,dic_id_second):\n",
    "        if cat_id in second_keys:\n",
    "            return cat_id\n",
    "        elif cat_id in second_values:\n",
    "            return Find_key_by_value_12many(v,dic_id_second)\n",
    "\n",
    "    def Find_second_importance_by_secondhex(second_hex,dic_second_id,u,step,second_importance,b,batch_size):\n",
    "        second_values=dic_second_id.values()\n",
    "        second_keys=dic_second_id.keys()\n",
    "        second_id=Find_key_by_value_121(second_hex,second_values,second_keys)#second_hec2second_id\n",
    "        importance=second_importance[b*(len_outputs)+u,step,second_id]#second_id2importance\n",
    "        return importance\n",
    "    \n",
    "    cat_reindex_id=list(dic_cat_id.values())\n",
    "    cat_id=list(dic_cat_id.keys())\n",
    "    second_keys=list(dic_id_second.keys())\n",
    "    second_values=[[x for x in sublist] for sublist in list(dic_id_second.values())]\n",
    "\n",
    "    len_outputs=len(outputs)\n",
    "    output_with_importance=[[[0 for _ in step] for step in u] for u in outputs]\n",
    "    \n",
    "    for ui,u in enumerate(outputs):#修改每个用户\n",
    "        for stepi,step in enumerate(u):#对用户每个时间步修改\n",
    "            for ci,c in enumerate(step):#对每个预测位置修改，已经reindex\n",
    "                if ci in cat_reindex_id:#当预测索引在所有目录重索引中时\n",
    "                    cat_id=Find_key_by_value(ci,values,keys)#取对应的cat_id\n",
    "                    cat_second_id=Find_second_by_cat_id(cat_id,second_keys,dic_id_second)#将对应id(hex)映射到二级目录(hex)\n",
    "                    importance=Find_second_importance_by_secondhex(cat_second_id,dic_second_id,ui,stepi,second_importance,b,batch_size)#查找该二级目录重要性\n",
    "                    outputs_with_importance[ui,stepi,ci]=outputs[ui,stepi,ci]*importance#更新output对应位置值\n",
    "    return outputs_with_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde27802-2022-43d9-b6a4-6dd00d309321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.用second_importance直接做权重\n",
    "#放入epoch训练\n",
    "for epoch in range(epoch_size):\n",
    "    rec_pre_model.train()\n",
    "    epoch_loss=0.0\n",
    "    \n",
    "    top_k_1=0\n",
    "    top_k_5=0\n",
    "    top_k_10=0\n",
    "    top_k_20=0\n",
    "    data_len=len(pre_data)\n",
    "    \n",
    "    for batch_step,(batch_users,batch_cat_id,batch_delta_t,\n",
    "              batch_delta_d,batch_tgt_cat_id) in enumerate(pre_data):\n",
    "        '''\n",
    "        batch_users：批中用户编号，未reindex\n",
    "        '''\n",
    "        #将batch放进模型\n",
    "        rec_pre_model.zero_grad()\n",
    "        cat_id_candidate=torch.arange(size_cat)#待预测对象,可改为poi_candidate\n",
    "\n",
    "        # #将数据放到cuda\n",
    "        batch_users=batch_users.cuda()\n",
    "        batch_cat_id=batch_cat_id.cuda()\n",
    "        batch_delta_t=batch_delta_t.cuda()\n",
    "        batch_delta_d=batch_delta_d.cuda()\n",
    "\n",
    "        outputs=rec_pre_model(batch_users,batch_cat_id,batch_delta_t,batch_delta_d)\n",
    "        #计算一个批次的损失\n",
    "        '''print(outputs.size())#(5,20,285)\n",
    "        print(batch_tgt_cat_id.size())#(5,20)'''\n",
    "        loss=0\n",
    "        #3对每个用户计算真实的时间步\n",
    "        batch_outputs_list=Regain_batch_outputs_len(batch_step,outputs)\n",
    "        batch_outputs_list_with_importance=Checkin_cat2importance(batch_outputs_list,dic_cat_id,dic_id_second,dic_second_id,second_importance,batch_step,batch_size)\n",
    "        #3.1去得每个用户的nonpadded打卡cat_id\n",
    "        batch_nonpadded_tgt_cat_id,_=Regain_batch_tgt(batch_step,batch_tgt_cat_id)\n",
    "        for i in range(batch_users.size(0)):#i是每一个用户\n",
    "            '''batch_output[i] (seq_len,candidate_size)  batch_out(batch_size,seq_len,candidate_size)\n",
    "            batch_nonpadded_tgt_cat_id[i] (seq_len,1)  batch_nonpadded_tgt_cat_id(batch_size,seqlen,candidate_size)'''\n",
    "            loss+=loss_function(batch_outputs_list_with_importance[i],torch.tensor(batch_nonpadded_tgt_cat_id[i]).squeeze().cuda())\n",
    "\n",
    "        loss.backward()#每个batch做一次反向传播\n",
    "        optimizer.step()\n",
    "        epoch_loss+=float(loss)#每个epoch的损失\n",
    "\n",
    "        #准备评价数据\n",
    "        outputs_evaluation=[sliced_tensor[-1,:] for sliced_tensor in batch_outputs_list_with_importance]#batch_outputs_list[:,-1,:]\n",
    "        tgt_evaluation=[sliced_tensor[-1] for sliced_tensor in batch_nonpadded_tgt_cat_id]#batch_nonpadded_tgt_cat_id[:,-1]\n",
    "\n",
    "        out_p,indices=torch.sort(torch.stack(outputs_evaluation),dim=1,descending=True)\n",
    "        count=float(len(batch_users))\n",
    "        #计算评价指标\n",
    "        top_k_1+=Top_k_precision(indices,tgt_evaluation,1)\n",
    "        top_k_5+=Top_k_precision(indices,tgt_evaluation,5)\n",
    "        top_k_10+=Top_k_precision(indices,tgt_evaluation,10)\n",
    "        top_k_20+=Top_k_precision(indices,tgt_evaluation,20)\n",
    "        print(batch_step)\n",
    "    print(\n",
    "        'epoch:[{}/{}]\\t'.format(epoch,epoch_size),\n",
    "        'loss:{:.4f}\\t'.format(epoch_loss),\n",
    "        'top@1:{:4f}\\t'.format(top_k_1/data_len),\n",
    "        'top@5:{:4f}\\t'.format(top_k_5/data_len),\n",
    "        'top@10:{:4f}\\t'.format(top_k_10/data_len),\n",
    "        'top@20:{:4f}\\t'.format(top_k_20/data_len)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfdf69a2-acfc-4c61-aca9-dac67173fe4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[0/25]\t loss:4992.3030\t top@1:0.057407\t top@5:0.206173\t top@10:0.343210\t top@20:0.497531\t\n",
      "epoch:[1/25]\t loss:4991.3906\t top@1:0.053704\t top@5:0.203395\t top@10:0.346914\t top@20:0.495679\t\n",
      "epoch:[2/25]\t loss:4991.8419\t top@1:0.052778\t top@5:0.207099\t top@10:0.345062\t top@20:0.501235\t\n",
      "epoch:[3/25]\t loss:4990.7917\t top@1:0.056481\t top@5:0.206173\t top@10:0.345679\t top@20:0.491975\t\n",
      "epoch:[4/25]\t loss:4992.0190\t top@1:0.056481\t top@5:0.208025\t top@10:0.349691\t top@20:0.504938\t\n",
      "epoch:[5/25]\t loss:4991.0357\t top@1:0.059259\t top@5:0.206481\t top@10:0.346914\t top@20:0.495679\t\n",
      "epoch:[6/25]\t loss:4993.3340\t top@1:0.053704\t top@5:0.202469\t top@10:0.344136\t top@20:0.487346\t\n",
      "epoch:[7/25]\t loss:4991.7227\t top@1:0.066667\t top@5:0.204321\t top@10:0.347840\t top@20:0.500309\t\n",
      "epoch:[8/25]\t loss:4992.2841\t top@1:0.059259\t top@5:0.207099\t top@10:0.349691\t top@20:0.500309\t\n",
      "epoch:[9/25]\t loss:4991.5673\t top@1:0.059259\t top@5:0.208025\t top@10:0.343210\t top@20:0.493827\t\n",
      "epoch:[10/25]\t loss:4995.1508\t top@1:0.056481\t top@5:0.207099\t top@10:0.342284\t top@20:0.492901\t\n",
      "epoch:[11/25]\t loss:4993.2965\t top@1:0.052778\t top@5:0.208025\t top@10:0.344136\t top@20:0.487346\t\n",
      "epoch:[12/25]\t loss:4990.7017\t top@1:0.054630\t top@5:0.208025\t top@10:0.346914\t top@20:0.497531\t\n",
      "epoch:[13/25]\t loss:4992.1341\t top@1:0.057407\t top@5:0.203395\t top@10:0.343210\t top@20:0.499383\t\n",
      "epoch:[14/25]\t loss:4991.5050\t top@1:0.049074\t top@5:0.206173\t top@10:0.347840\t top@20:0.496605\t\n",
      "epoch:[15/25]\t loss:4992.1085\t top@1:0.056481\t top@5:0.207099\t top@10:0.336728\t top@20:0.496605\t\n",
      "epoch:[16/25]\t loss:4989.7743\t top@1:0.055556\t top@5:0.203395\t top@10:0.344136\t top@20:0.499383\t\n",
      "epoch:[17/25]\t loss:4990.1486\t top@1:0.052778\t top@5:0.205247\t top@10:0.344136\t top@20:0.494753\t\n",
      "epoch:[18/25]\t loss:4990.9749\t top@1:0.050000\t top@5:0.204321\t top@10:0.346914\t top@20:0.500309\t\n",
      "epoch:[19/25]\t loss:4992.6201\t top@1:0.055556\t top@5:0.205247\t top@10:0.344753\t top@20:0.496605\t\n",
      "epoch:[20/25]\t loss:4990.3606\t top@1:0.058333\t top@5:0.203395\t top@10:0.354938\t top@20:0.497531\t\n",
      "epoch:[21/25]\t loss:4990.6907\t top@1:0.050000\t top@5:0.205247\t top@10:0.354321\t top@20:0.504012\t\n",
      "epoch:[22/25]\t loss:4991.3495\t top@1:0.057407\t top@5:0.206173\t top@10:0.344136\t top@20:0.494753\t\n",
      "epoch:[23/25]\t loss:4991.7279\t top@1:0.059259\t top@5:0.215432\t top@10:0.350309\t top@20:0.486420\t\n",
      "epoch:[24/25]\t loss:4993.5707\t top@1:0.062037\t top@5:0.211728\t top@10:0.345062\t top@20:0.497531\t\n"
     ]
    }
   ],
   "source": [
    "#放入epoch训练\n",
    "for epoch in range(epoch_size):\n",
    "    rec_pre_model.train()\n",
    "    epoch_loss=0.0\n",
    "    \n",
    "    top_k_1=0\n",
    "    top_k_5=0\n",
    "    top_k_10=0\n",
    "    top_k_20=0\n",
    "    data_len=len(pre_data)\n",
    "    \n",
    "    for batch_step,(batch_users,batch_cat_id,batch_delta_t,\n",
    "              batch_delta_d,batch_tgt_cat_id) in enumerate(pre_data):\n",
    "        '''\n",
    "        batch_users：批中用户编号，未reindex\n",
    "        '''\n",
    "        #将batch放进模型\n",
    "        rec_pre_model.zero_grad()\n",
    "        cat_id_candidate=torch.arange(size_cat)#待预测对象,可改为poi_candidate\n",
    "\n",
    "        # #将数据放到cuda\n",
    "        batch_users=batch_users.cuda()\n",
    "        batch_cat_id=batch_cat_id.cuda()\n",
    "        batch_delta_t=batch_delta_t.cuda()\n",
    "        batch_delta_d=batch_delta_d.cuda()\n",
    "\n",
    "        outputs=rec_pre_model(batch_users,batch_cat_id,batch_delta_t,batch_delta_d)\n",
    "        #计算一个批次的损失\n",
    "        '''print(outputs.size())#(5,20,285)\n",
    "        print(batch_tgt_cat_id.size())#(5,20)'''\n",
    "        loss=0\n",
    "        #3对每个用户计算真实的时间步\n",
    "        batch_outputs_list=Regain_batch_outputs_len(batch_step,outputs)\n",
    "        #3.1去得每个用户的nonpadded打卡cat_id\n",
    "        batch_nonpadded_tgt_cat_id,_=Regain_batch_tgt(batch_step,batch_tgt_cat_id)\n",
    "        for i in range(batch_users.size(0)):#i是每一个用户\n",
    "            '''batch_output[i] (seq_len,candidate_size)  batch_out(batch_size,seq_len,candidate_size)\n",
    "            batch_nonpadded_tgt_cat_id[i] (seq_len,1)  batch_nonpadded_tgt_cat_id(batch_size,seqlen,candidate_size)'''\n",
    "            loss+=loss_function(batch_outputs_list[i],torch.tensor(batch_nonpadded_tgt_cat_id[i]).squeeze().cuda())\n",
    "\n",
    "        loss.backward()#每个batch做一次反向传播\n",
    "        optimizer.step()\n",
    "        epoch_loss+=float(loss)#每个epoch的损失\n",
    "\n",
    "        #准备评价数据\n",
    "        outputs_evaluation=[sliced_tensor[-1,:] for sliced_tensor in batch_outputs_list]#batch_outputs_list[:,-1,:]\n",
    "        tgt_evaluation=[sliced_tensor[-1] for sliced_tensor in batch_nonpadded_tgt_cat_id]#batch_nonpadded_tgt_cat_id[:,-1]\n",
    "\n",
    "        out_p,indices=torch.sort(torch.stack(outputs_evaluation),dim=1,descending=True)\n",
    "        count=float(len(batch_users))\n",
    "        #计算评价指标\n",
    "        top_k_1+=Top_k_precision(indices,tgt_evaluation,1)\n",
    "        top_k_5+=Top_k_precision(indices,tgt_evaluation,5)\n",
    "        top_k_10+=Top_k_precision(indices,tgt_evaluation,10)\n",
    "        top_k_20+=Top_k_precision(indices,tgt_evaluation,20)\n",
    "    print(\n",
    "        'epoch:[{}/{}]\\t'.format(epoch,epoch_size),\n",
    "        'loss:{:.4f}\\t'.format(epoch_loss),\n",
    "        'top@1:{:4f}\\t'.format(top_k_1/data_len),\n",
    "        'top@5:{:4f}\\t'.format(top_k_5/data_len),\n",
    "        'top@10:{:4f}\\t'.format(top_k_10/data_len),\n",
    "        'top@20:{:4f}\\t'.format(top_k_20/data_len)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770f9edb-7691-47e1-b8df-81a8f4f1a686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[0/25]\t loss:8962.0209\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000926\t\n",
      "epoch:[1/25]\t loss:8961.9524\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[2/25]\t loss:8959.4588\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[3/25]\t loss:8959.8643\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000926\t\n",
      "epoch:[4/25]\t loss:8961.4076\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000926\t\n",
      "epoch:[5/25]\t loss:8961.4715\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[6/25]\t loss:8961.3275\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000926\t\n",
      "epoch:[7/25]\t loss:8961.2702\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[8/25]\t loss:8959.0940\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[9/25]\t loss:8960.1312\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000926\t\n",
      "epoch:[10/25]\t loss:8960.8960\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000926\t\n",
      "epoch:[11/25]\t loss:8962.5236\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[12/25]\t loss:8958.8759\t top@1:0.000000\t top@5:0.000000\t top@10:0.000926\t top@20:0.001852\t\n",
      "epoch:[13/25]\t loss:8962.4822\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[14/25]\t loss:8961.3026\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[15/25]\t loss:8962.7300\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[16/25]\t loss:8959.2086\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[17/25]\t loss:8961.8012\t top@1:0.000000\t top@5:0.000926\t top@10:0.000926\t top@20:0.000926\t\n",
      "epoch:[18/25]\t loss:8961.0540\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[19/25]\t loss:8959.3433\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000926\t\n",
      "epoch:[20/25]\t loss:8961.9592\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[21/25]\t loss:8960.1804\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[22/25]\t loss:8961.2788\t top@1:0.000000\t top@5:0.000000\t top@10:0.001852\t top@20:0.001852\t\n",
      "epoch:[23/25]\t loss:8962.3229\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000000\t\n",
      "epoch:[24/25]\t loss:8960.0605\t top@1:0.000000\t top@5:0.000000\t top@10:0.000000\t top@20:0.000926\t\n"
     ]
    }
   ],
   "source": [
    "#4重新训练模型\n",
    "#放入epoch训练\n",
    "rec_pre_model_poi=RecPreferenceModel(embed_size_user,embed_size_cat,hidden_size,\n",
    "size_user,size_cat,size_poi,num_layers,size_poi)\n",
    "rec_pre_model_poi.cuda()\n",
    "pre_data_poi=Process_rec(users,padded_reindex_rec_cat_id,\n",
    "                                  padded_rec_delta_t,padded_rec_delta_d,\n",
    "                                  padded_tgt_reindex_rec_poi,batch_size)\n",
    "for epoch in range(epoch_size):\n",
    "    rec_pre_model_poi.train()\n",
    "    epoch_loss=0.0\n",
    "    \n",
    "    top_k_1=0\n",
    "    top_k_5=0\n",
    "    top_k_10=0\n",
    "    top_k_20=0\n",
    "    data_len=len(pre_data_poi)\n",
    "    \n",
    "    for batch_step,(batch_users,batch_cat_id,batch_delta_t,\n",
    "              batch_delta_d,batch_tgt_poi) in enumerate(pre_data_poi):\n",
    "        '''\n",
    "        batch_users：批中用户编号，未reindex\n",
    "        '''\n",
    "        #将batch放进模型\n",
    "        rec_pre_model.zero_grad()\n",
    "        poi_candidate=torch.arange(size_poi)#待预测对象,可改为poi/candidate\n",
    "\n",
    "        # #将数据放到cuda\n",
    "        batch_users=batch_users.cuda()\n",
    "        batch_cat_id=batch_cat_id.cuda()\n",
    "        batch_delta_t=batch_delta_t.cuda()\n",
    "        batch_delta_d=batch_delta_d.cuda()\n",
    "\n",
    "        outputs=rec_pre_model_poi(batch_users,batch_cat_id,batch_delta_t,batch_delta_d)\n",
    "        #计算一个批次的损失\n",
    "        '''print(outputs.size())#(5,20,285)\n",
    "        print(batch_tgt_cat_id.size())#(5,20)'''\n",
    "        loss=0\n",
    "        #3对每个用户计算真实的时间步\n",
    "        batch_outputs_list=Regain_batch_outputs_len(batch_step,outputs)\n",
    "        #3.1去得每个用户的nonpadded打卡cat_id\n",
    "        batch_nonpadded_tgt_poi,_=Regain_batch_tgt(batch_step,batch_tgt_poi)\n",
    "        for i in range(batch_users.size(0)):#i是每一个用户\n",
    "            '''batch_output[i] (seq_len,candidate_size)  batch_out(batch_size,seq_len,candidate_size)\n",
    "            batch_nonpadded_tgt_cat_id[i] (seq_len,1)  batch_nonpadded_tgt_cat_id(batch_size,seqlen,candidate_size)'''\n",
    "            loss+=loss_function(batch_outputs_list[i],torch.tensor(batch_nonpadded_tgt_poi[i]).squeeze().cuda())\n",
    "\n",
    "        loss.backward()#每个batch做一次反向传播\n",
    "        optimizer.step()\n",
    "        epoch_loss+=float(loss)#每个epoch的损失\n",
    "\n",
    "        #准备评价数据\n",
    "        outputs_evaluation=[sliced_tensor[-1,:] for sliced_tensor in batch_outputs_list]#batch_outputs_list[:,-1,:]\n",
    "        tgt_evaluation=[sliced_tensor[-1] for sliced_tensor in batch_nonpadded_tgt_poi]#batch_nonpadded_tgt_cat_id[:,-1]\n",
    "\n",
    "        out_p,indices=torch.sort(torch.stack(outputs_evaluation),dim=1,descending=True)\n",
    "        count=float(len(batch_users))\n",
    "        #计算评价指标\n",
    "        top_k_1+=Top_k_precision(indices,tgt_evaluation,1)\n",
    "        top_k_5+=Top_k_precision(indices,tgt_evaluation,5)\n",
    "        top_k_10+=Top_k_precision(indices,tgt_evaluation,10)\n",
    "        top_k_20+=Top_k_precision(indices,tgt_evaluation,20)\n",
    "    print(\n",
    "        'epoch:[{}/{}]\\t'.format(epoch,epoch_size),\n",
    "        'loss:{:.4f}\\t'.format(epoch_loss),\n",
    "        'top@1:{:4f}\\t'.format(top_k_1/data_len),\n",
    "        'top@5:{:4f}\\t'.format(top_k_5/data_len),\n",
    "        'top@10:{:4f}\\t'.format(top_k_10/data_len),\n",
    "        'top@20:{:4f}\\t'.format(top_k_20/data_len)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56773d5-a0d4-4b01-93bc-8557b1c590af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
